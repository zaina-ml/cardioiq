{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqZ_UQHetoO8",
        "outputId": "3714cf00-e4b6-46c1-dd34-305c0d0f22e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.3 wfdb-4.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDVlJh65sYfg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wfdb\n",
        "import random\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import amp\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "from sklearn.metrics import average_precision_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainCfg:\n",
        "    window_sec = 2.0\n",
        "    crop_len = 360\n",
        "    records = tuple([\n",
        "        '100','101','102','103','104','105','106','107','108','109','111','112','113','114','115',\n",
        "        '116','117','118','119','121','122','123','124','200','201','202','203','205','207','208',\n",
        "        '209','210','212','213','214','215','217','219','220','221','222','223','228','230','231',\n",
        "        '232','233','234'\n",
        "    ])\n",
        "    label_keep = (\"N\", \"V\", \"F\")\n",
        "    split_mode = \"stratified\"\n",
        "    train_ratio = 0.7\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15\n",
        "    crop_len: int = 720\n",
        "    batch_size: int = 128\n",
        "    max_epochs: int = 50\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    use_sampler: bool = True\n",
        "    sampler_scale: float = 0.2\n",
        "    focal_gamma: float = 1.5\n",
        "    hybrid_switch_epoch: int = 15\n",
        "    augment: bool = True\n",
        "    severity: int = 0.25\n",
        "    clip_grad_norm: float = 1.0\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    amp: bool = True\n",
        "    seed: int = 42\n",
        "\n",
        "CFG = TrainCfg()\n",
        "\n",
        "torch.manual_seed(CFG.seed)\n",
        "np.random.seed(CFG.seed)\n",
        "random.seed(CFG.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmiS5zzTtD1b"
      },
      "outputs": [],
      "source": [
        "def load_record(name, duration_sec=600):\n",
        "    rec = wfdb.rdrecord(name, pn_dir=\"mitdb\")\n",
        "    ann = wfdb.rdann(name, \"atr\", pn_dir=\"mitdb\")\n",
        "    fs = rec.fs\n",
        "    sig = rec.p_signal[:int(fs*duration_sec)]\n",
        "    valid = ann.sample < int(fs*duration_sec)\n",
        "    ann.sample = ann.sample[valid]\n",
        "    ann.symbol = np.array(ann.symbol)[valid].tolist()\n",
        "    return sig, ann, fs\n",
        "\n",
        "def segment_beats(signal, ann, fs, window_sec, label_keep):\n",
        "    half = int(fs*window_sec/2)\n",
        "    beats, labels = [], []\n",
        "    for sample, sym in zip(ann.sample, ann.symbol):\n",
        "        if sym not in label_keep:\n",
        "            continue\n",
        "        start, end = sample-half, sample+half\n",
        "        if start<0 or end>len(signal):\n",
        "            continue\n",
        "        seg = signal[start:end,0]\n",
        "        seg = (seg - np.mean(seg))/(np.std(seg)+1e-8)\n",
        "        beats.append(seg.astype(np.float32))\n",
        "        labels.append(0 if sym==\"N\" else 1)\n",
        "    return np.array(beats), np.array(labels)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def build_dataset(cfg):\n",
        "    if getattr(cfg, \"split_mode\", \"record\") == \"record\":\n",
        "        rng = np.random.default_rng(cfg.seed)\n",
        "        records = list(cfg.records)\n",
        "        rng.shuffle(records)\n",
        "        n_total = len(records)\n",
        "        n_train = int(n_total * cfg.train_ratio)\n",
        "        n_val = int(n_total * cfg.val_ratio)\n",
        "        train_recs = records[:n_train]\n",
        "        val_recs = records[n_train:n_train + n_val]\n",
        "        test_recs = records[n_train + n_val:]\n",
        "\n",
        "        def proc(rec_list):\n",
        "            xs, ys = [], []\n",
        "            for r in rec_list:\n",
        "                sig, ann, fs = load_record(r)\n",
        "                x, y = segment_beats(sig, ann, fs, cfg.window_sec, cfg.label_keep)\n",
        "                if x.size == 0 or y.size == 0:\n",
        "                    continue\n",
        "                if x.ndim == 1:\n",
        "                    x = x[None, :]\n",
        "                xs.append(x[:, None, :])\n",
        "                ys.append(y)\n",
        "            if len(xs) == 0:\n",
        "                return np.empty((0, 1, cfg.crop_len)), np.empty((0,), dtype=int)\n",
        "            return np.concatenate(xs, axis=0), np.concatenate(ys, axis=0)\n",
        "\n",
        "        X_train, y_train = proc(train_recs)\n",
        "        X_val, y_val = proc(val_recs)\n",
        "        X_test, y_test = proc(test_recs)\n",
        "\n",
        "    else:\n",
        "        xs, ys = [], []\n",
        "        for r in cfg.records:\n",
        "            sig, ann, fs = load_record(r)\n",
        "            x, y = segment_beats(sig, ann, fs, cfg.window_sec, cfg.label_keep)\n",
        "            if x.size == 0 or y.size == 0:\n",
        "                continue\n",
        "            if x.ndim == 1:\n",
        "                x = x[None, :]\n",
        "            xs.append(x[:, None, :])\n",
        "            ys.append(y)\n",
        "        X = np.concatenate(xs, axis=0)\n",
        "        y = np.concatenate(ys, axis=0)\n",
        "\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "            X, y,\n",
        "            test_size=cfg.val_ratio + cfg.test_ratio,\n",
        "            stratify=y,\n",
        "            random_state=cfg.seed\n",
        "        )\n",
        "        rel_test = cfg.test_ratio / (cfg.val_ratio + cfg.test_ratio)\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=rel_test,\n",
        "            stratify=y_temp,\n",
        "            random_state=cfg.seed\n",
        "        )\n",
        "    print(\"Class distribution:\")\n",
        "    for name, labels in zip([\"Train\", \"Val\", \"Test\"], [y_train, y_val, y_test]):\n",
        "        if len(labels) > 0:\n",
        "            ratio = np.mean(labels)\n",
        "            print(f\"{name}: {len(labels)} samples | pos ratio {ratio:.4f}\")\n",
        "        else:\n",
        "            print(f\"{name}: 0 samples\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMbJq4CbtLqn",
        "outputId": "157ed61a-67a1-4ad9-fe44-c45e26124d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution:\n",
            "Train: 19371 samples | pos ratio 0.0936\n",
            "Val: 4151 samples | pos ratio 0.0937\n",
            "Test: 4151 samples | pos ratio 0.0935\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_val, y_val), (X_test, y_test) = build_dataset(CFG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEXHto4cs4ib"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class AugmentationScheduler:\n",
        "    def __init__(self, prog_epochs=10, max_severity=None):\n",
        "        self.prog_epochs = prog_epochs\n",
        "        self.max_severity = max_severity\n",
        "\n",
        "    def get(self, epoch, y):\n",
        "        base = min(1.0, epoch / max(1, self.prog_epochs))\n",
        "        severity = base * self.max_severity\n",
        "\n",
        "        return min(severity, self.max_severity)\n",
        "\n",
        "\n",
        "class AddGaussianNoise(nn.Module):\n",
        "    def __init__(self, std_factor=0.05):\n",
        "        super().__init__()\n",
        "        self.std_factor = std_factor\n",
        "\n",
        "    def forward(self, x, severity):\n",
        "        std = torch.std(x) * self.std_factor * severity * 10\n",
        "        return x + torch.randn_like(x) * std\n",
        "\n",
        "\n",
        "class AmplitudeScale(nn.Module):\n",
        "    def __init__(self, scale=0.1):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x, severity):\n",
        "        factor = 1 + self.scale * (2 * torch.rand(1, device=x.device) - 1) * severity\n",
        "        return x * factor\n",
        "\n",
        "\n",
        "class TimeWarp(nn.Module):\n",
        "    def __init__(self, max_stretch=0.05):\n",
        "        super().__init__()\n",
        "        self.max_stretch = max_stretch\n",
        "\n",
        "    def forward(self, x, severity):\n",
        "        L = x.shape[-1]\n",
        "\n",
        "        stretch = 1 + (torch.rand(1, device=x.device) * 2 - 1) * self.max_stretch * severity\n",
        "        new_len = max(2, int(L * stretch.item()))\n",
        "\n",
        "        x_in = x.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        warped = F.interpolate(x_in, size=new_len, mode=\"linear\", align_corners=False)\n",
        "        resampled = F.interpolate(warped, size=L, mode=\"linear\", align_corners=False)\n",
        "\n",
        "        return resampled.squeeze(0).squeeze(0)\n",
        "\n",
        "\n",
        "class RandomShift(nn.Module):\n",
        "    def __init__(self, max_shift=0.05):\n",
        "        super().__init__()\n",
        "        self.max_shift = max_shift\n",
        "\n",
        "    def forward(self, x, severity):\n",
        "        L = x.shape[-1]\n",
        "        max_shift = int(L * self.max_shift * severity)\n",
        "        if max_shift > 0:\n",
        "            k = int(torch.randint(-max_shift, max_shift + 1, (1,), device=x.device))\n",
        "            x = torch.roll(x, shifts=k, dims=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RandomDropout(nn.Module):\n",
        "    def __init__(self, max_frac=0.05):\n",
        "        super().__init__()\n",
        "        self.max_frac = max_frac\n",
        "\n",
        "    def forward(self, x, severity):\n",
        "        L = x.shape[-1]\n",
        "        chunk_len = max(1, int(L * self.max_frac * severity))\n",
        "        if chunk_len > 0:\n",
        "            start = int(torch.randint(0, max(1, L - chunk_len), (1,), device=x.device))\n",
        "            x[start:start + chunk_len] = 0.0\n",
        "        return x\n",
        "\n",
        "\n",
        "class RandomSpike(nn.Module):\n",
        "    def __init__(self, max_amp=0.2):\n",
        "        super().__init__()\n",
        "        self.max_amp = max_amp\n",
        "\n",
        "    def forward(self, x, severity):\n",
        "        L = x.shape[-1]\n",
        "        amp = self.max_amp * severity * torch.std(x)\n",
        "        at = int(torch.randint(0, L, (1,), device=x.device))\n",
        "        l = min(int(torch.randint(1, 4, (1,), device=x.device)), L - at)\n",
        "        x[at:at + l] += torch.randn(l, device=x.device) * amp\n",
        "        return x\n",
        "\n",
        "\n",
        "class ECGAugment(nn.Module):\n",
        "    def __init__(self, transforms):\n",
        "        super().__init__()\n",
        "        self.transforms = nn.ModuleList(transforms)\n",
        "\n",
        "    def forward(self, x, severity):\n",
        "        chosen = random.sample(list(self.transforms), random.randint(1, len(self.transforms)))\n",
        "        for t in chosen:\n",
        "            x = t(x, severity)\n",
        "        return torch.clamp(x, -5, 5)\n",
        "\n",
        "\n",
        "class ECGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y, augment=False, cfg=None):\n",
        "        self.X = torch.tensor(X.squeeze(), dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.augment = augment\n",
        "        self.severity = cfg.severity\n",
        "\n",
        "        self.scheduler = AugmentationScheduler(\n",
        "            max_severity=self.severity,\n",
        "        )\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.augment_pipeline = ECGAugment([\n",
        "            AddGaussianNoise(),\n",
        "            AmplitudeScale(),\n",
        "            TimeWarp(),\n",
        "            RandomShift(),\n",
        "            RandomDropout(),\n",
        "            RandomSpike()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.X[idx], self.y[idx]\n",
        "        if self.augment:\n",
        "            severity = self.scheduler.get(self.current_epoch, int(y.item()))\n",
        "            x = self.augment_pipeline(x.to(self.cfg.device), severity)\n",
        "\n",
        "        return x.unsqueeze(0), y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGOgKuU6szW-"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=5, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel, stride=stride, padding=kernel//2)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel, padding=kernel//2)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "        self.down = None\n",
        "        if in_ch != out_ch or stride != 1:\n",
        "            self.down = nn.Conv1d(in_ch, out_ch, 1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x if self.down is None else self.down(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = F.relu(out + identity)\n",
        "        return out\n",
        "\n",
        "class SEBlock1D(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = x.mean(dim=2)\n",
        "        w = F.relu(self.fc1(w))\n",
        "        w = torch.sigmoid(self.fc2(w)).unsqueeze(-1)\n",
        "        return x * w\n",
        "\n",
        "\n",
        "class MultiScaleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv3 = nn.Conv1d(in_ch, out_ch, 3, padding=1)\n",
        "        self.conv5 = nn.Conv1d(in_ch, out_ch, 5, padding=2)\n",
        "        self.conv7 = nn.Conv1d(in_ch, out_ch, 7, padding=3)\n",
        "        self.bn = nn.BatchNorm1d(out_ch*3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x3 = self.conv3(x)\n",
        "        x5 = self.conv5(x)\n",
        "        x7 = self.conv7(x)\n",
        "        out = torch.cat([x3, x5, x7], dim=1)\n",
        "        return F.relu(self.bn(out))\n",
        "\n",
        "\n",
        "class ECGNet(nn.Module):\n",
        "    def __init__(self, input_len=720, dropout_p=0.5, temporal_context=False, lstm_hidden=256):\n",
        "        super().__init__()\n",
        "        self.temporal_context = temporal_context\n",
        "\n",
        "        self.stem = nn.Conv1d(1, 64, 7, padding=3)\n",
        "\n",
        "        self.r1 = ResidualBlock(64, 128)\n",
        "        self.r2 = ResidualBlock(128, 256)\n",
        "        self.r3 = ResidualBlock(256, 256)\n",
        "        self.r4 = ResidualBlock(256, 512)\n",
        "        self.se = SEBlock1D(512)\n",
        "\n",
        "        self.ms = MultiScaleConv(512, 256)\n",
        "\n",
        "        self.lstm = None\n",
        "        if temporal_context:\n",
        "            self.lstm = nn.LSTM(input_size=256*3, hidden_size=lstm_hidden,\n",
        "                                batch_first=True, bidirectional=True)\n",
        "            self.fc = nn.Linear(lstm_hidden*2, 1)\n",
        "        else:\n",
        "            self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "            self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
        "            self.fc = nn.Linear(256*3*2, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.r1(x)\n",
        "        x = self.r2(x)\n",
        "        x = self.r3(x)\n",
        "        x = self.r4(x)\n",
        "        x = self.se(x)\n",
        "        x = self.ms(x)\n",
        "\n",
        "        if self.temporal_context:\n",
        "            B, T, C, L = x.shape\n",
        "            x = x.view(B, T, -1)\n",
        "            x, _ = self.lstm(x)\n",
        "            x = x[:, -1, :]\n",
        "        else:\n",
        "            a = self.avgpool(x).squeeze(-1)\n",
        "            m = self.maxpool(x).squeeze(-1)\n",
        "            x = torch.cat([a, m], dim=1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x).squeeze(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA5MjXnoswvo"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, focal_gamma=None, pos_weight=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.focal_gamma = focal_gamma\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, logits, targets, epoch=0):\n",
        "        prob = torch.sigmoid(logits)\n",
        "        bce = F.binary_cross_entropy_with_logits(\n",
        "            logits, targets, reduction=\"none\"\n",
        "        )\n",
        "        p_t = prob * targets + (1 - prob) * (1 - targets)\n",
        "        focal = ((1 - p_t) ** self.focal_gamma) * bce\n",
        "        if self.pos_weight is not None:\n",
        "            w = torch.where(targets == 1, self.pos_weight, torch.tensor(1.0, device=targets.device))\n",
        "            focal = focal * w\n",
        "        return focal.mean()\n",
        "\n",
        "class BCEWeighted(nn.Module):\n",
        "    def __init__(self, pos_weight=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, logits, targets, epoch=0):\n",
        "        return F.binary_cross_entropy_with_logits(\n",
        "            logits, targets, pos_weight=self.pos_weight, reduction=\"mean\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6335VnqstsY"
      },
      "outputs": [],
      "source": [
        "def tune_threshold_from_probs(y_true: np.ndarray, probs: np.ndarray):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, probs)\n",
        "    f1s = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "    best_idx = int(np.nanargmax(f1s))\n",
        "\n",
        "    if best_idx >= len(thresholds):\n",
        "        return 0.5, f1s[best_idx]\n",
        "    return float(thresholds[best_idx]), float(f1s[best_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_NwCcCFskTk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model: nn.Module,\n",
        "                X_train: np.ndarray, y_train: np.ndarray,\n",
        "                X_val: np.ndarray, y_val: np.ndarray,\n",
        "                cfg):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "\n",
        "    train_ds = ECGDataset(X_train, y_train, augment=True, cfg=cfg)\n",
        "    val_ds   = ECGDataset(X_val,   y_val, augment=False, cfg=cfg)\n",
        "\n",
        "    labels_flat = y_train.astype(int).flatten()\n",
        "    class_counts = np.bincount(labels_flat)\n",
        "    print(\"Class counts:\", class_counts)\n",
        "\n",
        "    if cfg.use_sampler:\n",
        "        weights = 1. / class_counts\n",
        "        sample_weights = weights[y_train.astype(int)]\n",
        "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, sampler=sampler)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    pos_weight = torch.tensor(\n",
        "        (len(y_train) - y_train.sum()) / max(1, y_train.sum()),\n",
        "        dtype=torch.float32,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    criterion = FocalLoss(focal_gamma=cfg.focal_gamma, pos_weight=pos_weight)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=10,\n",
        "        T_mult=2,\n",
        "        eta_min=cfg.lr * 1e-3\n",
        "    )\n",
        "\n",
        "    scaler = amp.GradScaler(\"cuda\", enabled=cfg.amp and device.startswith(\"cuda\"))\n",
        "\n",
        "    best_f1, best_state = -1.0, None\n",
        "    smoothed_thresh = 0.5\n",
        "    alpha_thresh = 0.3\n",
        "\n",
        "    for epoch in range(1, cfg.max_epochs + 1):\n",
        "        train_ds.current_epoch = epoch\n",
        "        model.train()\n",
        "        total_loss, n_seen = 0.0, 0\n",
        "        severity_sum = 0.0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device).view(-1)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_severity = np.mean([train_ds.scheduler.get(epoch, int(y.item())) for y in yb])\n",
        "            severity_sum += batch_severity * xb.size(0)\n",
        "\n",
        "            with amp.autocast('cuda', enabled=scaler.is_enabled()):\n",
        "                logits = model(xb).view(-1)\n",
        "                loss = criterion(logits, yb, epoch)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += float(loss.item()) * xb.size(0)\n",
        "            n_seen += xb.size(0)\n",
        "\n",
        "        scheduler.step(epoch + 1)\n",
        "\n",
        "        train_loss = total_loss / max(1, n_seen)\n",
        "        avg_severity = severity_sum / max(1, n_seen)\n",
        "\n",
        "        model.eval()\n",
        "        all_probs, all_y = [], []\n",
        "        with torch.inference_mode(), amp.autocast('cuda', enabled=scaler.is_enabled()):\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                logits = model(xb).view(-1)\n",
        "                probs = torch.clamp(torch.sigmoid(logits), min=1e-7, max=1 - 1e-7).cpu().numpy()\n",
        "                all_probs.append(probs)\n",
        "                all_y.append(yb.numpy().astype(int).flatten())\n",
        "\n",
        "        all_probs = np.concatenate(all_probs)\n",
        "        all_y = np.concatenate(all_y)\n",
        "\n",
        "        pr_auc = float(average_precision_score(all_y, all_probs))\n",
        "        roc_auc = float(roc_auc_score(all_y, all_probs))\n",
        "        epoch_thresh, epoch_f1 = tune_threshold_from_probs(all_y, all_probs)\n",
        "\n",
        "        preds = (all_probs >= epoch_thresh).astype(int)\n",
        "        epoch_precision = precision_score(all_y, preds, zero_division=0)\n",
        "        epoch_recall = recall_score(all_y, preds, zero_division=0)\n",
        "\n",
        "        smoothed_thresh = alpha_thresh * epoch_thresh + (1 - alpha_thresh) * smoothed_thresh\n",
        "\n",
        "        if epoch_f1 > best_f1:\n",
        "            best_f1 = epoch_f1\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            ckpt_path = os.path.join(cfg.save_dir, f\"best_f1_epoch{epoch:03d}_f1{best_f1:.4f}.pt\")\n",
        "            torch.save({\n",
        "                'model_state': best_state,\n",
        "                'epoch': epoch,\n",
        "                'f1': best_f1,\n",
        "                'threshold': smoothed_thresh\n",
        "            }, ckpt_path)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | train_loss {train_loss:.4f} \"\n",
        "              f\"| best_F1 {best_f1:.4f} @thr {smoothed_thresh:.3f} \"\n",
        "              f\"(Precision {epoch_precision:.3f} | Recall {epoch_recall:.3f} | PR-AUC {pr_auc:.3f} | ROC-AUC {roc_auc:.3f} | AvgSeverity {avg_severity:.3f})\")\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    print(f\"Training done. Best F1: {best_f1:.4f} | Smoothed threshold: {smoothed_thresh:.3f}\")\n",
        "    return model, smoothed_thresh, best_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G05omwZbNslM",
        "outputId": "78b73a0a-f942-4fc9-b23f-7b9b3f7b2c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class counts: [17557  1814]\n",
            "Epoch 001 | train_loss 0.7092 | best_F1 0.5644 @thr 0.595 (Precision 0.422 | Recall 0.851 | PR-AUC 0.486 | ROC-AUC 0.932 | AvgSeverity 0.025)\n",
            "Epoch 002 | train_loss 0.3390 | best_F1 0.5891 @thr 0.663 (Precision 0.456 | Recall 0.833 | PR-AUC 0.483 | ROC-AUC 0.935 | AvgSeverity 0.050)\n",
            "Epoch 003 | train_loss 0.2973 | best_F1 0.5917 @thr 0.722 (Precision 0.436 | Recall 0.920 | PR-AUC 0.493 | ROC-AUC 0.942 | AvgSeverity 0.075)\n",
            "Epoch 004 | train_loss 0.2810 | best_F1 0.5917 @thr 0.784 (Precision 0.455 | Recall 0.828 | PR-AUC 0.481 | ROC-AUC 0.941 | AvgSeverity 0.100)\n",
            "Epoch 005 | train_loss 0.2608 | best_F1 0.6301 @thr 0.821 (Precision 0.509 | Recall 0.828 | PR-AUC 0.494 | ROC-AUC 0.945 | AvgSeverity 0.125)\n",
            "Epoch 006 | train_loss 0.2474 | best_F1 0.6301 @thr 0.860 (Precision 0.510 | Recall 0.823 | PR-AUC 0.571 | ROC-AUC 0.952 | AvgSeverity 0.150)\n",
            "Epoch 007 | train_loss 0.2331 | best_F1 0.6392 @thr 0.872 (Precision 0.509 | Recall 0.859 | PR-AUC 0.574 | ROC-AUC 0.953 | AvgSeverity 0.175)\n",
            "Epoch 008 | train_loss 0.2125 | best_F1 0.6659 @thr 0.890 (Precision 0.598 | Recall 0.751 | PR-AUC 0.596 | ROC-AUC 0.957 | AvgSeverity 0.200)\n",
            "Epoch 009 | train_loss 0.2091 | best_F1 0.6761 @thr 0.896 (Precision 0.586 | Recall 0.799 | PR-AUC 0.620 | ROC-AUC 0.959 | AvgSeverity 0.225)\n",
            "Epoch 010 | train_loss 0.2930 | best_F1 0.6761 @thr 0.880 (Precision 0.479 | Recall 0.918 | PR-AUC 0.516 | ROC-AUC 0.946 | AvgSeverity 0.250)\n",
            "Epoch 011 | train_loss 0.2607 | best_F1 0.6761 @thr 0.898 (Precision 0.514 | Recall 0.820 | PR-AUC 0.522 | ROC-AUC 0.947 | AvgSeverity 0.250)\n",
            "Epoch 012 | train_loss 0.2478 | best_F1 0.6761 @thr 0.900 (Precision 0.497 | Recall 0.820 | PR-AUC 0.558 | ROC-AUC 0.951 | AvgSeverity 0.250)\n",
            "Epoch 013 | train_loss 0.2543 | best_F1 0.6761 @thr 0.903 (Precision 0.538 | Recall 0.846 | PR-AUC 0.609 | ROC-AUC 0.957 | AvgSeverity 0.250)\n",
            "Epoch 014 | train_loss 0.2444 | best_F1 0.6761 @thr 0.878 (Precision 0.516 | Recall 0.830 | PR-AUC 0.578 | ROC-AUC 0.954 | AvgSeverity 0.250)\n",
            "Epoch 015 | train_loss 0.2443 | best_F1 0.6761 @thr 0.856 (Precision 0.532 | Recall 0.751 | PR-AUC 0.639 | ROC-AUC 0.954 | AvgSeverity 0.250)\n",
            "Epoch 016 | train_loss 0.2277 | best_F1 0.6761 @thr 0.858 (Precision 0.508 | Recall 0.913 | PR-AUC 0.604 | ROC-AUC 0.958 | AvgSeverity 0.250)\n",
            "Epoch 017 | train_loss 0.2111 | best_F1 0.6761 @thr 0.845 (Precision 0.521 | Recall 0.910 | PR-AUC 0.644 | ROC-AUC 0.960 | AvgSeverity 0.250)\n",
            "Epoch 018 | train_loss 0.2193 | best_F1 0.6761 @thr 0.860 (Precision 0.547 | Recall 0.851 | PR-AUC 0.647 | ROC-AUC 0.959 | AvgSeverity 0.250)\n",
            "Epoch 019 | train_loss 0.1896 | best_F1 0.6761 @thr 0.873 (Precision 0.586 | Recall 0.774 | PR-AUC 0.653 | ROC-AUC 0.960 | AvgSeverity 0.250)\n",
            "Epoch 020 | train_loss 0.2021 | best_F1 0.6995 @thr 0.887 (Precision 0.590 | Recall 0.859 | PR-AUC 0.676 | ROC-AUC 0.965 | AvgSeverity 0.250)\n",
            "Epoch 021 | train_loss 0.1836 | best_F1 0.7000 @thr 0.887 (Precision 0.597 | Recall 0.846 | PR-AUC 0.672 | ROC-AUC 0.965 | AvgSeverity 0.250)\n",
            "Epoch 022 | train_loss 0.1738 | best_F1 0.7118 @thr 0.890 (Precision 0.619 | Recall 0.838 | PR-AUC 0.698 | ROC-AUC 0.968 | AvgSeverity 0.250)\n",
            "Epoch 023 | train_loss 0.1697 | best_F1 0.7118 @thr 0.883 (Precision 0.584 | Recall 0.869 | PR-AUC 0.732 | ROC-AUC 0.970 | AvgSeverity 0.250)\n",
            "Epoch 024 | train_loss 0.1587 | best_F1 0.7125 @thr 0.896 (Precision 0.682 | Recall 0.746 | PR-AUC 0.754 | ROC-AUC 0.972 | AvgSeverity 0.250)\n",
            "Epoch 025 | train_loss 0.1580 | best_F1 0.7264 @thr 0.903 (Precision 0.686 | Recall 0.771 | PR-AUC 0.743 | ROC-AUC 0.972 | AvgSeverity 0.250)\n",
            "Epoch 026 | train_loss 0.1492 | best_F1 0.7455 @thr 0.902 (Precision 0.702 | Recall 0.794 | PR-AUC 0.773 | ROC-AUC 0.974 | AvgSeverity 0.250)\n",
            "Epoch 027 | train_loss 0.1477 | best_F1 0.7455 @thr 0.904 (Precision 0.705 | Recall 0.779 | PR-AUC 0.765 | ROC-AUC 0.974 | AvgSeverity 0.250)\n",
            "Epoch 028 | train_loss 0.1406 | best_F1 0.7455 @thr 0.908 (Precision 0.709 | Recall 0.784 | PR-AUC 0.773 | ROC-AUC 0.975 | AvgSeverity 0.250)\n",
            "Epoch 029 | train_loss 0.1467 | best_F1 0.7497 @thr 0.912 (Precision 0.720 | Recall 0.781 | PR-AUC 0.779 | ROC-AUC 0.975 | AvgSeverity 0.250)\n",
            "Epoch 030 | train_loss 0.2236 | best_F1 0.7497 @thr 0.918 (Precision 0.630 | Recall 0.787 | PR-AUC 0.703 | ROC-AUC 0.967 | AvgSeverity 0.250)\n",
            "Epoch 031 | train_loss 0.2039 | best_F1 0.7497 @thr 0.885 (Precision 0.551 | Recall 0.861 | PR-AUC 0.668 | ROC-AUC 0.961 | AvgSeverity 0.250)\n",
            "Epoch 032 | train_loss 0.1943 | best_F1 0.7497 @thr 0.894 (Precision 0.636 | Recall 0.751 | PR-AUC 0.678 | ROC-AUC 0.961 | AvgSeverity 0.250)\n",
            "Epoch 033 | train_loss 0.2043 | best_F1 0.7497 @thr 0.903 (Precision 0.692 | Recall 0.728 | PR-AUC 0.734 | ROC-AUC 0.970 | AvgSeverity 0.250)\n",
            "Epoch 034 | train_loss 0.1908 | best_F1 0.7497 @thr 0.900 (Precision 0.523 | Recall 0.910 | PR-AUC 0.676 | ROC-AUC 0.963 | AvgSeverity 0.250)\n",
            "Epoch 035 | train_loss 0.1864 | best_F1 0.7497 @thr 0.894 (Precision 0.593 | Recall 0.838 | PR-AUC 0.708 | ROC-AUC 0.967 | AvgSeverity 0.250)\n",
            "Epoch 036 | train_loss 0.1970 | best_F1 0.7497 @thr 0.908 (Precision 0.588 | Recall 0.877 | PR-AUC 0.697 | ROC-AUC 0.967 | AvgSeverity 0.250)\n",
            "Epoch 037 | train_loss 0.1856 | best_F1 0.7497 @thr 0.914 (Precision 0.586 | Recall 0.802 | PR-AUC 0.706 | ROC-AUC 0.965 | AvgSeverity 0.250)\n",
            "Epoch 038 | train_loss 0.1912 | best_F1 0.7497 @thr 0.919 (Precision 0.658 | Recall 0.761 | PR-AUC 0.754 | ROC-AUC 0.970 | AvgSeverity 0.250)\n",
            "Epoch 039 | train_loss 0.1699 | best_F1 0.7497 @thr 0.915 (Precision 0.647 | Recall 0.758 | PR-AUC 0.740 | ROC-AUC 0.968 | AvgSeverity 0.250)\n",
            "Epoch 040 | train_loss 0.1826 | best_F1 0.7497 @thr 0.916 (Precision 0.595 | Recall 0.853 | PR-AUC 0.709 | ROC-AUC 0.967 | AvgSeverity 0.250)\n",
            "Epoch 041 | train_loss 0.1768 | best_F1 0.7497 @thr 0.908 (Precision 0.577 | Recall 0.879 | PR-AUC 0.701 | ROC-AUC 0.967 | AvgSeverity 0.250)\n",
            "Epoch 042 | train_loss 0.1612 | best_F1 0.7497 @thr 0.917 (Precision 0.639 | Recall 0.799 | PR-AUC 0.722 | ROC-AUC 0.969 | AvgSeverity 0.250)\n",
            "Epoch 043 | train_loss 0.1673 | best_F1 0.7497 @thr 0.911 (Precision 0.615 | Recall 0.874 | PR-AUC 0.757 | ROC-AUC 0.972 | AvgSeverity 0.250)\n",
            "Epoch 044 | train_loss 0.1648 | best_F1 0.7609 @thr 0.915 (Precision 0.688 | Recall 0.851 | PR-AUC 0.796 | ROC-AUC 0.977 | AvgSeverity 0.250)\n",
            "Epoch 045 | train_loss 0.1580 | best_F1 0.7609 @thr 0.917 (Precision 0.683 | Recall 0.805 | PR-AUC 0.775 | ROC-AUC 0.975 | AvgSeverity 0.250)\n",
            "Epoch 046 | train_loss 0.1570 | best_F1 0.7609 @thr 0.913 (Precision 0.646 | Recall 0.820 | PR-AUC 0.784 | ROC-AUC 0.974 | AvgSeverity 0.250)\n",
            "Epoch 047 | train_loss 0.1594 | best_F1 0.7609 @thr 0.907 (Precision 0.699 | Recall 0.794 | PR-AUC 0.783 | ROC-AUC 0.976 | AvgSeverity 0.250)\n",
            "Epoch 048 | train_loss 0.1493 | best_F1 0.7609 @thr 0.916 (Precision 0.744 | Recall 0.746 | PR-AUC 0.799 | ROC-AUC 0.977 | AvgSeverity 0.250)\n",
            "Epoch 049 | train_loss 0.1470 | best_F1 0.7637 @thr 0.915 (Precision 0.677 | Recall 0.877 | PR-AUC 0.808 | ROC-AUC 0.978 | AvgSeverity 0.250)\n",
            "Epoch 050 | train_loss 0.1416 | best_F1 0.7637 @thr 0.921 (Precision 0.757 | Recall 0.722 | PR-AUC 0.815 | ROC-AUC 0.977 | AvgSeverity 0.250)\n",
            "Epoch 051 | train_loss 0.1415 | best_F1 0.7649 @thr 0.909 (Precision 0.727 | Recall 0.807 | PR-AUC 0.785 | ROC-AUC 0.976 | AvgSeverity 0.250)\n",
            "Epoch 052 | train_loss 0.1389 | best_F1 0.7649 @thr 0.912 (Precision 0.667 | Recall 0.861 | PR-AUC 0.805 | ROC-AUC 0.978 | AvgSeverity 0.250)\n",
            "Epoch 053 | train_loss 0.1307 | best_F1 0.7744 @thr 0.912 (Precision 0.744 | Recall 0.807 | PR-AUC 0.818 | ROC-AUC 0.979 | AvgSeverity 0.250)\n",
            "Epoch 054 | train_loss 0.1317 | best_F1 0.7744 @thr 0.908 (Precision 0.685 | Recall 0.843 | PR-AUC 0.824 | ROC-AUC 0.979 | AvgSeverity 0.250)\n",
            "Epoch 055 | train_loss 0.1257 | best_F1 0.7748 @thr 0.899 (Precision 0.709 | Recall 0.853 | PR-AUC 0.838 | ROC-AUC 0.981 | AvgSeverity 0.250)\n",
            "Epoch 056 | train_loss 0.1290 | best_F1 0.7803 @thr 0.898 (Precision 0.719 | Recall 0.853 | PR-AUC 0.835 | ROC-AUC 0.980 | AvgSeverity 0.250)\n",
            "Epoch 057 | train_loss 0.1182 | best_F1 0.7971 @thr 0.888 (Precision 0.744 | Recall 0.859 | PR-AUC 0.858 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 058 | train_loss 0.1240 | best_F1 0.7971 @thr 0.876 (Precision 0.695 | Recall 0.884 | PR-AUC 0.854 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 059 | train_loss 0.1127 | best_F1 0.7971 @thr 0.888 (Precision 0.773 | Recall 0.805 | PR-AUC 0.853 | ROC-AUC 0.982 | AvgSeverity 0.250)\n",
            "Epoch 060 | train_loss 0.1116 | best_F1 0.7971 @thr 0.887 (Precision 0.727 | Recall 0.848 | PR-AUC 0.848 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 061 | train_loss 0.1069 | best_F1 0.7971 @thr 0.890 (Precision 0.752 | Recall 0.843 | PR-AUC 0.862 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 062 | train_loss 0.1064 | best_F1 0.7971 @thr 0.892 (Precision 0.753 | Recall 0.846 | PR-AUC 0.856 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 063 | train_loss 0.1029 | best_F1 0.8019 @thr 0.877 (Precision 0.752 | Recall 0.859 | PR-AUC 0.859 | ROC-AUC 0.982 | AvgSeverity 0.250)\n",
            "Epoch 064 | train_loss 0.1063 | best_F1 0.8019 @thr 0.888 (Precision 0.777 | Recall 0.825 | PR-AUC 0.871 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 065 | train_loss 0.1034 | best_F1 0.8019 @thr 0.880 (Precision 0.726 | Recall 0.887 | PR-AUC 0.868 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 066 | train_loss 0.0999 | best_F1 0.8019 @thr 0.879 (Precision 0.727 | Recall 0.877 | PR-AUC 0.871 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 067 | train_loss 0.1020 | best_F1 0.8019 @thr 0.869 (Precision 0.720 | Recall 0.887 | PR-AUC 0.875 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 068 | train_loss 0.0993 | best_F1 0.8019 @thr 0.872 (Precision 0.742 | Recall 0.859 | PR-AUC 0.875 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 069 | train_loss 0.0896 | best_F1 0.8019 @thr 0.875 (Precision 0.747 | Recall 0.851 | PR-AUC 0.875 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 070 | train_loss 0.1603 | best_F1 0.8019 @thr 0.890 (Precision 0.661 | Recall 0.866 | PR-AUC 0.823 | ROC-AUC 0.979 | AvgSeverity 0.250)\n",
            "Epoch 071 | train_loss 0.1469 | best_F1 0.8019 @thr 0.902 (Precision 0.691 | Recall 0.799 | PR-AUC 0.757 | ROC-AUC 0.976 | AvgSeverity 0.250)\n",
            "Epoch 072 | train_loss 0.1534 | best_F1 0.8019 @thr 0.909 (Precision 0.686 | Recall 0.843 | PR-AUC 0.819 | ROC-AUC 0.979 | AvgSeverity 0.250)\n",
            "Epoch 073 | train_loss 0.1480 | best_F1 0.8019 @thr 0.906 (Precision 0.735 | Recall 0.799 | PR-AUC 0.802 | ROC-AUC 0.978 | AvgSeverity 0.250)\n",
            "Epoch 074 | train_loss 0.1451 | best_F1 0.8019 @thr 0.911 (Precision 0.697 | Recall 0.828 | PR-AUC 0.800 | ROC-AUC 0.978 | AvgSeverity 0.250)\n",
            "Epoch 075 | train_loss 0.1467 | best_F1 0.8019 @thr 0.919 (Precision 0.678 | Recall 0.807 | PR-AUC 0.802 | ROC-AUC 0.977 | AvgSeverity 0.250)\n",
            "Epoch 076 | train_loss 0.1384 | best_F1 0.8019 @thr 0.921 (Precision 0.763 | Recall 0.753 | PR-AUC 0.822 | ROC-AUC 0.979 | AvgSeverity 0.250)\n",
            "Epoch 077 | train_loss 0.1406 | best_F1 0.8019 @thr 0.926 (Precision 0.768 | Recall 0.730 | PR-AUC 0.832 | ROC-AUC 0.979 | AvgSeverity 0.250)\n",
            "Epoch 078 | train_loss 0.1422 | best_F1 0.8019 @thr 0.894 (Precision 0.680 | Recall 0.843 | PR-AUC 0.798 | ROC-AUC 0.976 | AvgSeverity 0.250)\n",
            "Epoch 079 | train_loss 0.1441 | best_F1 0.8019 @thr 0.894 (Precision 0.615 | Recall 0.825 | PR-AUC 0.764 | ROC-AUC 0.972 | AvgSeverity 0.250)\n",
            "Epoch 080 | train_loss 0.1481 | best_F1 0.8019 @thr 0.907 (Precision 0.693 | Recall 0.848 | PR-AUC 0.798 | ROC-AUC 0.978 | AvgSeverity 0.250)\n",
            "Epoch 081 | train_loss 0.1417 | best_F1 0.8019 @thr 0.888 (Precision 0.669 | Recall 0.869 | PR-AUC 0.790 | ROC-AUC 0.978 | AvgSeverity 0.250)\n",
            "Epoch 082 | train_loss 0.1451 | best_F1 0.8019 @thr 0.891 (Precision 0.707 | Recall 0.820 | PR-AUC 0.830 | ROC-AUC 0.979 | AvgSeverity 0.250)\n",
            "Epoch 083 | train_loss 0.1395 | best_F1 0.8019 @thr 0.892 (Precision 0.699 | Recall 0.871 | PR-AUC 0.845 | ROC-AUC 0.981 | AvgSeverity 0.250)\n",
            "Epoch 084 | train_loss 0.1371 | best_F1 0.8019 @thr 0.909 (Precision 0.688 | Recall 0.823 | PR-AUC 0.826 | ROC-AUC 0.979 | AvgSeverity 0.250)\n",
            "Epoch 085 | train_loss 0.1296 | best_F1 0.8019 @thr 0.906 (Precision 0.722 | Recall 0.841 | PR-AUC 0.825 | ROC-AUC 0.980 | AvgSeverity 0.250)\n",
            "Epoch 086 | train_loss 0.1386 | best_F1 0.8019 @thr 0.915 (Precision 0.805 | Recall 0.774 | PR-AUC 0.857 | ROC-AUC 0.982 | AvgSeverity 0.250)\n",
            "Epoch 087 | train_loss 0.1326 | best_F1 0.8019 @thr 0.910 (Precision 0.730 | Recall 0.807 | PR-AUC 0.796 | ROC-AUC 0.978 | AvgSeverity 0.250)\n",
            "Epoch 088 | train_loss 0.1299 | best_F1 0.8019 @thr 0.893 (Precision 0.716 | Recall 0.861 | PR-AUC 0.848 | ROC-AUC 0.981 | AvgSeverity 0.250)\n",
            "Epoch 089 | train_loss 0.1329 | best_F1 0.8019 @thr 0.884 (Precision 0.760 | Recall 0.820 | PR-AUC 0.856 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 090 | train_loss 0.1263 | best_F1 0.8019 @thr 0.887 (Precision 0.658 | Recall 0.897 | PR-AUC 0.815 | ROC-AUC 0.980 | AvgSeverity 0.250)\n",
            "Epoch 091 | train_loss 0.1287 | best_F1 0.8019 @thr 0.897 (Precision 0.747 | Recall 0.848 | PR-AUC 0.846 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 092 | train_loss 0.1255 | best_F1 0.8019 @thr 0.907 (Precision 0.708 | Recall 0.774 | PR-AUC 0.815 | ROC-AUC 0.976 | AvgSeverity 0.250)\n",
            "Epoch 093 | train_loss 0.1257 | best_F1 0.8059 @thr 0.922 (Precision 0.841 | Recall 0.774 | PR-AUC 0.873 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 094 | train_loss 0.1203 | best_F1 0.8145 @thr 0.911 (Precision 0.766 | Recall 0.869 | PR-AUC 0.872 | ROC-AUC 0.985 | AvgSeverity 0.250)\n",
            "Epoch 095 | train_loss 0.1227 | best_F1 0.8145 @thr 0.908 (Precision 0.799 | Recall 0.797 | PR-AUC 0.862 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 096 | train_loss 0.1174 | best_F1 0.8145 @thr 0.911 (Precision 0.777 | Recall 0.799 | PR-AUC 0.848 | ROC-AUC 0.981 | AvgSeverity 0.250)\n",
            "Epoch 097 | train_loss 0.1184 | best_F1 0.8145 @thr 0.897 (Precision 0.750 | Recall 0.841 | PR-AUC 0.866 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 098 | train_loss 0.1183 | best_F1 0.8145 @thr 0.892 (Precision 0.761 | Recall 0.835 | PR-AUC 0.877 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 099 | train_loss 0.1060 | best_F1 0.8145 @thr 0.894 (Precision 0.739 | Recall 0.853 | PR-AUC 0.875 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 100 | train_loss 0.1197 | best_F1 0.8145 @thr 0.909 (Precision 0.816 | Recall 0.776 | PR-AUC 0.868 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 101 | train_loss 0.1128 | best_F1 0.8145 @thr 0.892 (Precision 0.721 | Recall 0.864 | PR-AUC 0.851 | ROC-AUC 0.983 | AvgSeverity 0.250)\n",
            "Epoch 102 | train_loss 0.1110 | best_F1 0.8145 @thr 0.902 (Precision 0.775 | Recall 0.815 | PR-AUC 0.857 | ROC-AUC 0.982 | AvgSeverity 0.250)\n",
            "Epoch 103 | train_loss 0.1002 | best_F1 0.8145 @thr 0.888 (Precision 0.727 | Recall 0.882 | PR-AUC 0.871 | ROC-AUC 0.982 | AvgSeverity 0.250)\n",
            "Epoch 104 | train_loss 0.1093 | best_F1 0.8145 @thr 0.890 (Precision 0.763 | Recall 0.846 | PR-AUC 0.872 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 105 | train_loss 0.1048 | best_F1 0.8145 @thr 0.886 (Precision 0.786 | Recall 0.820 | PR-AUC 0.874 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 106 | train_loss 0.1055 | best_F1 0.8145 @thr 0.890 (Precision 0.785 | Recall 0.846 | PR-AUC 0.881 | ROC-AUC 0.985 | AvgSeverity 0.250)\n",
            "Epoch 107 | train_loss 0.1030 | best_F1 0.8146 @thr 0.895 (Precision 0.800 | Recall 0.830 | PR-AUC 0.872 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 108 | train_loss 0.1081 | best_F1 0.8221 @thr 0.888 (Precision 0.759 | Recall 0.897 | PR-AUC 0.873 | ROC-AUC 0.985 | AvgSeverity 0.250)\n",
            "Epoch 109 | train_loss 0.1010 | best_F1 0.8221 @thr 0.896 (Precision 0.799 | Recall 0.810 | PR-AUC 0.865 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 110 | train_loss 0.0960 | best_F1 0.8221 @thr 0.893 (Precision 0.776 | Recall 0.864 | PR-AUC 0.893 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 111 | train_loss 0.0854 | best_F1 0.8221 @thr 0.897 (Precision 0.830 | Recall 0.779 | PR-AUC 0.878 | ROC-AUC 0.984 | AvgSeverity 0.250)\n",
            "Epoch 112 | train_loss 0.0977 | best_F1 0.8221 @thr 0.898 (Precision 0.782 | Recall 0.856 | PR-AUC 0.877 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 113 | train_loss 0.1022 | best_F1 0.8221 @thr 0.907 (Precision 0.834 | Recall 0.789 | PR-AUC 0.893 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 114 | train_loss 0.0906 | best_F1 0.8251 @thr 0.908 (Precision 0.792 | Recall 0.861 | PR-AUC 0.891 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 115 | train_loss 0.0883 | best_F1 0.8251 @thr 0.908 (Precision 0.834 | Recall 0.789 | PR-AUC 0.899 | ROC-AUC 0.985 | AvgSeverity 0.250)\n",
            "Epoch 116 | train_loss 0.0886 | best_F1 0.8251 @thr 0.900 (Precision 0.783 | Recall 0.846 | PR-AUC 0.889 | ROC-AUC 0.985 | AvgSeverity 0.250)\n",
            "Epoch 117 | train_loss 0.0872 | best_F1 0.8304 @thr 0.896 (Precision 0.806 | Recall 0.856 | PR-AUC 0.893 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 118 | train_loss 0.0833 | best_F1 0.8304 @thr 0.885 (Precision 0.780 | Recall 0.846 | PR-AUC 0.891 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 119 | train_loss 0.0873 | best_F1 0.8304 @thr 0.900 (Precision 0.812 | Recall 0.843 | PR-AUC 0.906 | ROC-AUC 0.988 | AvgSeverity 0.250)\n",
            "Epoch 120 | train_loss 0.0800 | best_F1 0.8304 @thr 0.887 (Precision 0.770 | Recall 0.879 | PR-AUC 0.889 | ROC-AUC 0.985 | AvgSeverity 0.250)\n",
            "Epoch 121 | train_loss 0.0805 | best_F1 0.8304 @thr 0.883 (Precision 0.760 | Recall 0.871 | PR-AUC 0.894 | ROC-AUC 0.985 | AvgSeverity 0.250)\n",
            "Epoch 122 | train_loss 0.0877 | best_F1 0.8304 @thr 0.874 (Precision 0.792 | Recall 0.843 | PR-AUC 0.895 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 123 | train_loss 0.0761 | best_F1 0.8304 @thr 0.876 (Precision 0.809 | Recall 0.848 | PR-AUC 0.906 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 124 | train_loss 0.0761 | best_F1 0.8321 @thr 0.882 (Precision 0.812 | Recall 0.853 | PR-AUC 0.909 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 125 | train_loss 0.0764 | best_F1 0.8321 @thr 0.894 (Precision 0.827 | Recall 0.823 | PR-AUC 0.902 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 126 | train_loss 0.0717 | best_F1 0.8321 @thr 0.902 (Precision 0.839 | Recall 0.805 | PR-AUC 0.907 | ROC-AUC 0.986 | AvgSeverity 0.250)\n",
            "Epoch 127 | train_loss 0.0737 | best_F1 0.8337 @thr 0.893 (Precision 0.795 | Recall 0.877 | PR-AUC 0.909 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 128 | train_loss 0.0760 | best_F1 0.8337 @thr 0.889 (Precision 0.801 | Recall 0.861 | PR-AUC 0.908 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 129 | train_loss 0.0690 | best_F1 0.8337 @thr 0.894 (Precision 0.843 | Recall 0.815 | PR-AUC 0.909 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 130 | train_loss 0.0698 | best_F1 0.8337 @thr 0.894 (Precision 0.842 | Recall 0.807 | PR-AUC 0.905 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 131 | train_loss 0.0684 | best_F1 0.8383 @thr 0.904 (Precision 0.844 | Recall 0.833 | PR-AUC 0.909 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 132 | train_loss 0.0661 | best_F1 0.8383 @thr 0.901 (Precision 0.837 | Recall 0.830 | PR-AUC 0.913 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 133 | train_loss 0.0686 | best_F1 0.8383 @thr 0.892 (Precision 0.804 | Recall 0.864 | PR-AUC 0.913 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 134 | train_loss 0.0643 | best_F1 0.8383 @thr 0.889 (Precision 0.821 | Recall 0.848 | PR-AUC 0.914 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 135 | train_loss 0.0663 | best_F1 0.8383 @thr 0.883 (Precision 0.820 | Recall 0.853 | PR-AUC 0.913 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 136 | train_loss 0.0636 | best_F1 0.8394 @thr 0.882 (Precision 0.846 | Recall 0.833 | PR-AUC 0.916 | ROC-AUC 0.988 | AvgSeverity 0.250)\n",
            "Epoch 137 | train_loss 0.0603 | best_F1 0.8394 @thr 0.879 (Precision 0.803 | Recall 0.861 | PR-AUC 0.912 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 138 | train_loss 0.0652 | best_F1 0.8394 @thr 0.868 (Precision 0.807 | Recall 0.848 | PR-AUC 0.910 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 139 | train_loss 0.0585 | best_F1 0.8394 @thr 0.879 (Precision 0.835 | Recall 0.835 | PR-AUC 0.914 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 140 | train_loss 0.0602 | best_F1 0.8394 @thr 0.878 (Precision 0.817 | Recall 0.861 | PR-AUC 0.915 | ROC-AUC 0.988 | AvgSeverity 0.250)\n",
            "Epoch 141 | train_loss 0.0630 | best_F1 0.8394 @thr 0.870 (Precision 0.807 | Recall 0.861 | PR-AUC 0.916 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 142 | train_loss 0.0597 | best_F1 0.8394 @thr 0.864 (Precision 0.810 | Recall 0.864 | PR-AUC 0.914 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 143 | train_loss 0.0575 | best_F1 0.8394 @thr 0.858 (Precision 0.817 | Recall 0.859 | PR-AUC 0.916 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 144 | train_loss 0.0582 | best_F1 0.8394 @thr 0.862 (Precision 0.829 | Recall 0.848 | PR-AUC 0.916 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 145 | train_loss 0.0559 | best_F1 0.8394 @thr 0.863 (Precision 0.824 | Recall 0.853 | PR-AUC 0.916 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 146 | train_loss 0.0560 | best_F1 0.8401 @thr 0.866 (Precision 0.830 | Recall 0.851 | PR-AUC 0.916 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 147 | train_loss 0.0572 | best_F1 0.8401 @thr 0.861 (Precision 0.809 | Recall 0.869 | PR-AUC 0.916 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 148 | train_loss 0.0590 | best_F1 0.8401 @thr 0.863 (Precision 0.830 | Recall 0.851 | PR-AUC 0.917 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 149 | train_loss 0.0569 | best_F1 0.8401 @thr 0.870 (Precision 0.833 | Recall 0.835 | PR-AUC 0.915 | ROC-AUC 0.987 | AvgSeverity 0.250)\n",
            "Epoch 150 | train_loss 0.1166 | best_F1 0.8401 @thr 0.860 (Precision 0.766 | Recall 0.843 | PR-AUC 0.876 | ROC-AUC 0.985 | AvgSeverity 0.250)\n",
            "Training done. Best F1: 0.8401 | Smoothed threshold: 0.860\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "          'max_epochs': 150 ,\n",
        "          'lr': 3e-4,\n",
        "          'weight_decay': 1e-04,\n",
        "          'focal_gamma': 1,\n",
        "          'crop_len': 720,\n",
        "          'severity': 0.25}\n",
        "\n",
        "cfg = TrainCfg()\n",
        "for k, v in params.items():\n",
        "    setattr(cfg, k, v)\n",
        "\n",
        "model = ECGNet(input_len=cfg.crop_len, dropout_p=0.3)\n",
        "\n",
        "trained_model, threshold, best_pr = train_model(\n",
        "    model, X_train, y_train, X_val, y_val, cfg\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    ECGDataset(X_test, y_test, augment=False, cfg=cfg),\n",
        "    batch_size=cfg.batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvXkojT1OzHZ"
      },
      "outputs": [],
      "source": [
        "trained_model.eval()\n",
        "\n",
        "torch.save(trained_model.state_dict(), \"cardioiq_model_v4.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9GH7rlvet8Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
